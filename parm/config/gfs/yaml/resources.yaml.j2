#####################################
# This jinja-yaml defines a resource dictionary for each task.
#
# Entries can be added to either the 'defaults' dictionary or valid run
# dictionaries (i.e. gdas, gfs, enkfgdas, enkfgfs) or both.  One
# additional run (nest) is available for fcst and efcs jobs.
# For any entry duplicated in defaults and a run dictionary, the run
# dictionary will override the defaults.
#
# When needed, subdictionaries can be defined for each component and resolution
# to any of the dictionaries.  These will likewise override anything defined
# therein.
# For example:
#####
# task:
#     defaults:
#         walltime: "01:00:00"
#         num_PEs = "atm_PEs + ocn_PEs"
#         atmres:
#             C768:
#                 atm_PEs: 100
#             C384:
#                 atm_PEs: 40
#             C192:
#                 atm_PEs: 40
#             C96:
#                 atm_PEs: 20
#             C48:
#                 atm_PEs: 10
#
#         ocnres:
#             025:
#                 ocn_PEs: 20
#             050:
#                 ocn_PEs: 10
#             100:
#                 ocn_PEs: 5
#             500:
#                 ocn_PEs: 2
##### End #####
#
# Valid components are "atm", "atmens", "ocn", "ice", "wave", and "aero".
# Note that tasks starting with "e" will key off of the ensemble resolution for
# the component.  All others will key off of the deterministic resolution.
#
# Required entries for each task are
#    num_PEs (integer or string) - Number of process elements.
#              Should be an integer if fixed.
#              If the number of PEs is dependent on variables defined in the
#              dictionary, then the string can be used to represent that
#              calculation.  For instance, if the dictionary defines a layout
#              layout_x: 6, layout_y: 6
#              and the task requires six times the product of layout_x and
#              layout_y, then the string can be set to
#              "layout_x * layout_y * 6"
#              The string must be valid Python syntax and all operators must have
#              spaces between them.
#
#    walltime (string) - Maximum time required.
#                        Format: "HH:MM:SS"
#
# Optional entries that are used referenced at setup time
#    mem_per_PE - Memory required per PE.
#                 Default: mem_per_core * threads
#                 Formats:
#                         - NNN(.N)NB where NNN.N are numbers and S is one of
#                           G (giga), M (mega), K (kilo)
#                         - "max" if all available memory should be used
#                         - "default"
#
#    PEs_per_node - Maximum number of tasks per node.  This will be adjusted
#                   downward at runtime if the memory requirement is high.
#
#    mem_per_node - memory required per node.
#                   If not defined, then mem_per_PE is used.
#                   Formats are identical to mem_per_PE.
#
#    threads - Number of openMP threads per PE.
#            - NOTE this is only for openMP and should not be
#                   used to adjust memory.  Use mem_per_PE instead.
#
#    adjustable_threads - Boolean whether the thread count can be adjusted
#
# Additional entries may also be added.  Doing so will prepend the key/value
#    pair to the appropriate config file.  If the entry is in defaults, it
#    will be added as
#         export <key>=<value>
#    If the entry is in a run-specific dictionary, it will be added as
#         if [[ ${RUN} == <run> ]]; then
#             export <key>=<value>
#         fi
#
##############################################

prep:
    defaults:
        walltime: "00:30:00"
        num_PEs: 4
        PEs_per_node: 2
        mem_per_node: "40GB"

prepsnowobs:
    defaults:
        walltime: "00:05:00"
        num_PEs: 1
        mem_per_node: "max"

prepatmiodaobs:
    defaults:
        walltime: "00:30:00"
        num_PEs: 1
        mem_per_node: "max"

aerosol_init:
    defaults:
        walltime: "00:05:00"
        num_PEs: 1
        mem_per_node: "6GB"

waveinit:
    defaults:
        walltime: "00:10:00"
        num_PEs: 12
        mem_per_PE: "200MB"

waveprep:
    defaults:
        walltime: "00:10:00"

    gfs:
        num_PEs: 65
        mem_per_PE: "2400MB"
    gdas:
        num_PEs: 5
        mem_per_PE: "20GB"

wavepostsbs:
    defaults:
        num_PEs: 8
        mem_per_PE: "1300MB"

    gdas:
        walltime: "00:20:00"
    gfs:
        walltime: "03:00:00"

wavepostbndpnt:
    defaults:
        walltime: "01:00:00"
        num_PEs: 240

wavepostbndpntbll:
    defaults:
        walltime: "01:00:00"
        num_PEs: 448

wavepostpnt:
    defaults:
        walltime: "04:00:00"
        num_PEs: 200
        PEs_per_node: 40

wavegempak:
    defaults:
        walltime: "02:00:00"
        num_PEs: 1
        mem_per_node: "1GB"

waveawipsbulls:
    defaults:
        walltime: "00:20:00"
        num_PEs: 1

waveawipsgridded:
    defaults:
        walltime: "02:00:00"
        num_PEs: 1
        mem_per_node: "1GB"

atmanlinit:
    defaults:
        walltime: "00:10:00"
        num_PEs: 1
        mem_per_node: "3GB"
        layout_x: @LAYOUT_X_ATMANL@
        layout_y: @LAYOUT_Y_ATMANL@
        layout_gsib_x: {{ @IO_LAYOUT_X@ * 3 }}
        layout_gsib_y: {{ @IO_LAYOUT_Y@ * 2 }}

atmanlvar:
    defaults:
        walltime: "00:30:00"
        num_PEs: {{ @LAYOUT_X_ATMANL@ * @LAYOUT_Y_ATMANL@ * 6 }}
        layout_x: @LAYOUT_X_ATMANL@
        layout_y: @LAYOUT_Y_ATMANL@
        layout_io_x: @IO_LAYOUT_X@
        layout_io_y: @IO_LAYOUT_Y@

atmanlfv3inc:
    defaults:
        walltime: "00:30:00"
        num_PEs: {{ @LAYOUT_X_ATMANL@ * @LAYOUT_Y_ATMANL@ * 6 }}
        layout_x: @LAYOUT_X_ATMANL@
        layout_y: @LAYOUT_Y_ATMANL@
        layout_io_x: @IO_LAYOUT_X@
        layout_io_y: @IO_LAYOUT_Y@

atmanlfinal:
    defaults:
        num_PEs: {{ host_info.cores_per_node }}
        walltime: "00:30:00"
        mem_per_node: "max"

snowanl:
    defaults:
        num_PEs: "layout_x * layout_y * 6"
        walltime: "00:15:00"
        layout_io_x: {{ @IO_LAYOUT_X@ }}
        layout_io_y: {{ @IO_LAYOUT_Y@ }}

        atmres:
            C768:
                layout_x: 6
                layout_y: 6

            C384:
                layout_x: 5
                layout_y: 5

            C192:
                layout_x: 1
                layout_y: 1

            C96:
                layout_x: 1
                layout_y: 1

            C48:
                layout_x: 1
                layout_y: 1

aeroanlinit:
    defaults:
        num_PEs: 1
        walltime: "00:10:00"
        mem_per_PE: "3GB"

        atmres:
            C768:
                layout_x: 8
                layout_y: 8

            C384:
                layout_x: 8
                layout_y: 8

            C192:
                layout_x: 8
                layout_y: 8

            C96:
                layout_x: 8
                layout_y: 8

            C48:
                layout_x: 1
                layout_y: 1

aeroanlrun:
    defaults:
        walltime: "00:30:00"
        num_PEs: "layout_x * layout_y * 6"

        atmres:
            C768:
                layout_x: 8
                layout_y: 8

            C384:
                layout_x: 8
                layout_y: 8

            C192:
                layout_x: 8
                layout_y: 8

            C96:
                layout_x: 8
                layout_y: 8

            C48:
                layout_x: 1
                layout_y: 1

aeroanlfinal:
    defaults:
        num_PEs: 1
        walltime: "00:10:00"
        mem_per_node: "3GB"

ocnanalprep:
    defaults:
        num_PEs: 1
        walltime: "00:10:00"
        mem_per_node: "24GB"

prepoceanobs:
    defaults:
        num_PEs: 1
        walltime: "00:10:00"
        mem_per_node: "48GB"

ocnanalbmat:
    defaults:
        walltime: "00:15:00"

        ocnres:
            025:
                num_PEs: 480
            050:
                num_PEs: 16
            500:
                num_PEs: 16

ocnanalrun:
    defaults:
        walltime: "00:15:00"

        ocnres:
            025:
                num_PEs: 480
                {% if machine == "jet" %}
                mem_per_node: "max"
                {% else %}
                mem_per_node: "96GB"
                {% endif %}
            050:
                num_PEs: 16
                {% if machine == "jet" %}
                mem_per_node: "max"
                {% else %}
                mem_per_node: "96GB"
                {% endif %}
            500:
                num_PEs: 16
                mem_per_node: "24GB"

ocnanalecen:
    defaults:
        walltime: "00:10:00"
        ocnres:
            025:
                num_PEs: 40
                {% if machine == "jet" %}
                mem_per_node: "max"
                {% else %}
                mem_per_node: "96GB"
                {% endif %}
            050:
                num_PEs: 16
                {% if machine == "jet" %}
                mem_per_node: "max"
                {% else %}
                mem_per_node: "96GB"
                {% endif %}
            500:
                num_PEs: 16
                mem_per_node: "24GB"

ocnanalletkf:
    defaults:
        walltime: "00:10:00"
        ocnres:
            025:
                num_PEs: 480
                {% if machine == "jet" %}
                mem_per_node: "max"
                {% else %}
                mem_per_node: "96GB"
                {% endif %}
            050:
                num_PEs: 16
                {% if machine == "jet" %}
                mem_per_node: "max"
                {% else %}
                mem_per_node: "96GB"
                {% endif %}
            500:
                num_PEs: 16
                mem_per_node: "24GB"

ocnanalchkpt:
    defaults:
        walltime: "00:10:00"
        ocnres:
            025:
                mem_per_PE: "3200MB"
                num_PEs: 40
            050:
                mem_per_node: "32GB"
                num_PEs: 16
            500:
                mem_per_node: "32GB"
                num_PEs: 8

ocnanalpost:
    defaults:
        walltime: "00:30:00"
        num_PEs: {{ host_info.cores_per_node }}

ocnanalvrfy:
    defaults:
        walltime: "00:35:00"
        num_PEs: 1
        mem_per_node: "24GB"

anal:
    defaults:
        mem_per_node: "max"
        # Defaults on all systems but S4
        threads: 5
        num_PEs: 84

        # Defaults for S4
        {% if machine == "s4" %}
        threads: 4
            {% if partition == "s4" %}
        num_PEs: 88
            {% elif partition == "ivy" %}
        num_PEs: 90
            {% endif %}
        {% endif %}

        atmres:
            # C768 threads/PEs are dependent on run
            C768: {}
            C384:
                {% if machine == "hera" or machine == "jet" %}
                threads: 8
                num_PEs: 270
                {% elif machine == "s4" %}
                threads: 1
                num_PEs: 416
                {% else %}
                threads: 10
                num_PEs: 160
                {% endif %}

            # Use defaults for C192, C96, and C48
            C192: {}
            C96: {}
            C48: {}

    gdas:
        wallclock: "01:20:00"
        atmres:
            C768:
                num_PEs: 780
                threads: 5
            # Use defaults for all other resolutions
            C384: {}
            C192: {}
            C96:  {}
            C48:  {}

    gfs:
        wallclock: "01:00:00"
        atmres:
            C768:
                num_PEs: 825
                threads: 5
            # Use defaults for all other resolutions
            C384: {}
            C192: {}
            C96:  {}
            C48:  {}

analcalc:
    # The analcalc job consists of a few sub jobs, each requiring a different
    # number of tasks.
    # echgres is run as part of analcalc and needs a different layout.
    #    - The number of tasks is the length of IAUFHRS.

    gdas:
        nth_echgres: 4

    gfs:
        # TODO Determine if this level of threading is really needed or just to resolve memory
        nth_echgres: 12

    defaults:
        walltime: "00:10:00"
        num_PEs: 127
        mem_per_PE: "1200MB"

analdiag:
    defaults:
        walltime: "00:15:00"
        num_PEs: 96             # Should be at least twice npe_ediag
        mem_per_PE: "1200MB"

sfcanl:
    defaults:
        walltime: "00:20:00"
        num_PEs: 6
        mem_per_PE: "15G"

oceanice_products:
    defaults:
        walltime: "00:15:00"
        num_PEs: 1
        mem_per_node: "max"

upp:
    defaults:
        walltime: "00:15:00"
        atmres:
            C768:
                num_PEs: 120
                mem_per_node: "max"
            C384:
                num_PEs: 120
                mem_per_node: "max"
            C192:
                num_PEs: 120
                mem_per_node: "max"
            C96:
                num_PEs: 96
            C48:
                num_PEs: 48

atmos_products:
    defaults:
        walltime: "00:15:00"
        num_PEs: 24
        mem_per_PE: "2600MB"

verfozn:
    defaults:
        walltime: "00:05:00"
        num_PEs: 1
        mem_per_node: "1GB"

verfrad:
    defaults:
        walltime: "00:40:00"
        num_PEs: 1
        mem_per_node: "5GB"

vminmon:
    defaults:
        walltime: "00:05:00"
        num_PEs: 1
        mem_per_node: "1GB"

tracker:
    defaults:
        walltime: "00:10:00"
        num_PEs: 1
        mem_per_node: "4GB"

genesis:
    defaults:
        walltime: "00:25:00"
        num_PEs: 1
        mem_per_node: "10GB"

genesis_fsu:
    defaults:
        walltime: "00:10:00"
        num_PEs: 1
        mem_per_node: "1GB"

fit2obs:
    defaults:
        walltime: "00:20:00"
        num_PEs: 3
        mem_per_node: "20GB"

metp:
    gdas:
        walltime: "03:00:00"
    gfs:
        walltime: "06:00:00"

    defaults:
        num_PEs: 4
        mem_per_PE: "20GB"

echgres:
    defaults:
        walltime: "00:10:00"
        num_PEs: 3
        mem_per_PE: "20GB"

init:
    defaults:
        walltime: "00:30:00"
        num_PEs: 24
        mem_per_PE: "12GB"

init_chem:
    defaults:
        walltime: "00:30:00"
        num_PEs: 1
        mem_per_node: "max"

mom6ic:
    defaults:
        walltime: "00:30:00"
        num_PEs: 24
        mem_per_PE: "3600MB"

arch:
    defaults:
        walltime: "06:00:00"
        num_PEs: 1
        mem_per_node: "4GB"

earc:
    defaults:
        walltime: "06:00:00"
        num_PEs: 1
        mem_per_node: "4GB"

getic:
    defaults:
        walltime: "06:00:00"
        num_PEs: 1
        mem_per_node: "4GB"

cleanup:
    defaults:
        walltime: "00:15:00"
        num_PEs: 1
        mem_per_node: "4GB"

stage_ic:
    defaults:
        walltime: "00:15:00"
        num_PEs: 1
        mem_per_node: "max"

atmensanlinit:
    defaults:
        layout_x: @LAYOUT_X_ATMENSANL@
        layout_y: @LAYOUT_Y_ATMENSANL@

        walltime: "00:10:00"
        num_PEs: 1
        mem_per_node: "3GB"

atmensanlrun:
    defaults:
        layout_x: @LAYOUT_X_ATMENSANL@
        layout_y: @LAYOUT_Y_ATMENSANL@

        walltime: "00:30:00"
        num_PEs: {{ @LAYOUT_X_ATMENSANL@ * @LAYOUT_Y_ATMENSANL@ * 6 }}

atmensanlfinal:
    defaults:
        walltime: "00:30:00"
        num_PEs: 1
        mem_per_node: "max"

eomg:
    defaults:
        walltime: "00:30:00"
        threads: 2
        atmensres:
            C384:
                num_PEs: 200
            C192:
                num_PEs: 100
            C96:
                num_PEs: 40
            C48:
                num_PEs: 40

eobs:
    defaults:
        walltime: "00:15:00"
        threads: 2

        atmensres:
            C384:
                num_PEs: 200
            C192:
                num_PEs: 100
            C96:
                num_PEs: 40
            C48:
                num_PEs: 40

ediag:
    defaults:
        walltime: "00:15:00"
        num_PEs: 48
        mem_per_PE: "1300MB"

eupd:
    defaults:
        walltime: "00:30:00"

        atmres:
            C768:
                mem_per_PE: "24GB"
                num_PEs: 480
            C384:
                mem_per_PE: "12GB"
                num_PEs: 270
            C96:
                mem_per_PE: "9GB"
                num_PEs: 42
            C48:
                mem_per_PE: "9GB"
                num_PEs: 42

ecen:
    defaults:
        walltime: "00:10:00"
        num_PEs: 80
        mem_per_PE: "4700MB"

        atmensres:
            C384:
                mem_per_PE: "16GB"
            C192:
                mem_per_PE: "4700MB"
            C96:
                mem_per_PE: "4700MB"
            C48:
                mem_per_PE: "4700MB"

esfc:
    defaults:
        walltime: "00:15:00"
        num_PEs: 80
        mem_per_node: "max"

epos:
    defaults:
        walltime: "00:15:00"
        num_PEs: 80

postsnd:
    defaults:
        walltime: "02:00:00"
        num_PEs: 40
        mem_per_PE: "9GB"
        num_PEs_postsndcfp: 9

awips:
    defaults:
        walltime: "03:30:00"
        num_PEs: 1
        mem_per_node: "3GB"

npoess:
    defaults:
        walltime: "03:30:00"
        num_PEs: 1
        mem_per_node: "3GB"

gempak:
    defaults:
        walltime: "03:00:00"

    gdas:
        num_PEs: 2
        mem_per_PE: "2GB"
    gfs:
        num_PEs: 28
        mem_per_PE: "100MB"

mos_stn_prep:
    defaults:
        walltime: "00:10:00"
        num_PEs: 3
        mem_per_PE: "2GB"
        PTILE: 3

mos_grd_prep:
    defaults:
        walltime: "00:10:00"
        num_PEs: 4
        mem_per_PE: "4GB"
        PTILE: 4

mos_ext_stn_prep:
    defaults:
        walltime: "00:15:00"
        num_PEs: 2
        mem_per_PE: "3GB"
        PTILE: 2

mos_ext_grd_prep:
    defaults:
        walltime: "00:10:00"
        num_PEs: 7
        mem_per_PE: "500MB"
        PTILE: 7

mos_stn_fcst:
    defaults:
        walltime: "00:10:00"
        num_PEs: 5
        mem_per_PE: "8GB"
        PTILE: 5

mos_grd_fcst:
    defaults:
        walltime: "00:10:00"
        num_PEs: 7
        mem_per_PE: "7GB"
        PTILE: 7

mos_ext_stn_fcst:
    defaults:
        walltime: "00:20:00"
        num_PEs: 3
        mem_per_PE: "16600MB"
        PTILE: 3
        prepost: True

mos_ext_grd_fcst:
    defaults:
        walltime: "00:10:00"
        num_PEs: 7
        mem_per_PE: "7GB"
        PTILE: 7

mos_stn_prdgen:
    defaults:
        walltime: "00:10:00"
        num_PEs: 1
        mem_per_node: "15GB"
        PTILE: 1
        prepost: True

mos_grd_prdgen:
    defaults:
        walltime: "00:40:00"
        num_PEs: 72
        threads: 4
        mem_per_PE: "2GB"
        PTILE: 72

mos_ext_stn_prdgen:
    defaults:
        walltime: "00:10:00"
        num_PEs: 1
        mem_per_node: "15GB"
        PTILE: 1
        prepost: True

mos_ext_grd_prdgen:
    defaults:
        walltime: "00:30:00"
        num_PEs: 96
        threads: 16
        mem_per_PE: "15GB"
        PTILE: 96

mos_wx_prdgen:
    defaults:
        walltime: "00:10:00"
        num_PEs: 4
        threads: 2
        mem_per_PE: "2600MB"
        PTILE: 4

mos_wx_ext_prdgen:
    defaults:
        walltime: "00:10:00"
        num_PEs: 4
        threads: 2
        mem_per_PE: "2600MB"
        PTILE: 4

#####################################################################
################## FORECAST-SPECIFIC RESOURCES ######################
#####################################################################

# Define macros to calculate UFS resources and write formatted yaml.
{% from "ufs_macros.yaml.j2" import fv3_resources %}

# Declare GDAS forecast resources for each resolution.
# These are the same for gdas (fcst) and enkfgdas (efcs) runs/jobs.
gdas_fcst_atm_by_res: &GdasFcstAtmRes
    C48:
        # Use the fv3_resources macro to calculate resources and fill the yaml
		{% filter indent(8) %}
		{{ fv3_resources(layout_x=1,
                         layout_y=1,
                         write_groups=1,
                         threads_fv3=1,
                         threads_ufs=1,
                         write_tasks_per_group_per_thread_per_tile=1) }}
		{% endfilter %}
        walltime: "00:20:00"
    C96:
		{% filter indent(8) %}
		{{ fv3_resources(layout_x=2,
                         layout_y=2,
                         write_groups=1,
                         threads_fv3=1,
                         threads_ufs=1,
                         write_tasks_per_group_per_thread_per_tile=1) }}
		{% endfilter %}
        walltime: "00:20:00"
    C192:
		{% filter indent(8) %}
		{{ fv3_resources(layout_x=4,
                         layout_y=6,
                         write_groups=1,
                         threads_fv3=1,
                         threads_ufs=1,
                         write_tasks_per_group_per_thread_per_tile=10) }}
		{% endfilter %}
        walltime: "00:20:00"
    C384:
		{% filter indent(8) %}
		{{ fv3_resources(layout_x=4,
                         layout_y=6,
                         write_groups=2,
                         threads_fv3=2,
                         threads_ufs=2,
                         write_tasks_per_group_per_thread_per_tile=10) }}
		{% endfilter %}
        walltime: "00:30:00"
    C768:
		{% filter indent(8) %}
		{{ fv3_resources(layout_x=8,
                         layout_y=12,
                         write_groups=2,
                         threads_fv3=4,
                         threads_ufs=4,
                         write_tasks_per_group_per_thread_per_tile=10) }}
		{% endfilter %}
        walltime: "00:30:00"
    C1152:
		{% filter indent(8) %}
		{{ fv3_resources(layout_x=8,
                         layout_y=16,
                         write_groups=4,
                         threads_fv3=4,
                         threads_ufs=4,
                         write_tasks_per_group_per_thread_per_tile=10) }}
		{% endfilter %}
        walltime: "00:30:00"
    C3072:
		{% filter indent(8) %}
		{{ fv3_resources(layout_x=16,
                         layout_y=32,
                         write_groups=4,
                         threads_fv3=4,
                         threads_ufs=4,
                         write_tasks_per_group_per_thread_per_tile=10) }}
		{% endfilter %}
        walltime: "00:30:00"

# Declare GFS forecast resources for each resolution.
# These are the same for gdas (fcst) and enkfgdas (efcs) runs/jobs.
gfs_fcst_atm_by_res: &GfsFcstAtmRes
    C48:
		{% filter indent(8) %}
		{{ fv3_resources(layout_x=1,
                         layout_y=1,
                         write_groups=1,
                         threads_fv3=1,
                         threads_ufs=1,
                         write_tasks_per_group_per_thread_per_tile=1) }}
		{% endfilter %}
        walltime: "03:00:00"
    C96:
		{% filter indent(8) %}
		{{ fv3_resources(layout_x=2,
                         layout_y=2,
                         write_groups=1,
                         threads_fv3=1,
                         threads_ufs=1,
                         write_tasks_per_group_per_thread_per_tile=1) }}
		{% endfilter %}
        walltime: "03:00:00"
    C192:
		{% filter indent(8) %}
		{{ fv3_resources(layout_x=4,
                         layout_y=6,
                         write_groups=2,
                         threads_fv3=2,
                         threads_ufs=2,
                         write_tasks_per_group_per_thread_per_tile=5) }}
		{% endfilter %}
        walltime: "03:00:00"
    C384:
		{% filter indent(8) %}
		{{ fv3_resources(layout_x=4,
                         layout_y=6,
                         write_groups=2,
                         threads_fv3=2,
                         threads_ufs=2,
                         write_tasks_per_group_per_thread_per_tile=10) }}
		{% endfilter %}
        walltime: "06:00:00"
    C768:
		{% filter indent(8) %}
		{{ fv3_resources(layout_x=12,
                         layout_y=16,
                         write_groups=4,
                         threads_fv3=4,
                         threads_ufs=4,
                         write_tasks_per_group_per_thread_per_tile=20) }}
		{% endfilter %}
        walltime: "06:00:00"
    C1152:
		{% filter indent(8) %}
		{{ fv3_resources(layout_x=8,
                         layout_y=16,
                         write_groups=4,
                         threads_fv3=4,
                         threads_ufs=4,
                         write_tasks_per_group_per_thread_per_tile=20) }}
		{% endfilter %}
        walltime: "06:00:00"
    C3072:
		{% filter indent(8) %}
		{{ fv3_resources(layout_x=16,
                         layout_y=32,
                         write_groups=4,
                         threads_fv3=4,
                         threads_ufs=4,
                         write_tasks_per_group_per_thread_per_tile=10) }}
		{% endfilter %}
        walltime: "06:00:00"

# Declare global GFS resources for a nested case.
nested_global_gfs_atm_fcst_by_res: &NestedGfsFcstAtmRes
    C96:
		{% filter indent(8) %}
		{{ fv3_resources(layout_x=4,
                         layout_y=4,
                         layout_x_nest=12,
                         layout_y_nest=10
                         write_groups=2,
                         threads_fv3=1,
                         threads_ufs=1,
                         write_tasks_per_group_per_thread_per_tile=2) }}
		{% endfilter %}
        walltime: "03:00:00"
    C192:
		{% filter indent(8) %}
		{{ fv3_resources(layout_x=5,
                         layout_y=6,
                         layout_x_nest=15,
                         layout_y_nest=25
                         write_groups=2,
                         threads_fv3=1,
                         threads_ufs=1,
                         write_tasks_per_group_per_thread_per_tile=15) }}
		{% endfilter %}
        walltime: "03:00:00"
    C384:
		{% filter indent(8) %}
		{{ fv3_resources(layout_x=8,
                         layout_y=8,
                         layout_x_nest=34,
                         layout_y_nest=24
                         write_groups=2,
                         threads_fv3=1,
                         threads_ufs=1,
                         write_tasks_per_group_per_thread_per_tile=20) }}
		{% endfilter %}
        walltime: "06:00:00"
    C768:
        layout_x: 48
        layout_y: 45
        write_groups: 2
        write_tasks_per_group_per_thread_per_tile: 90
        threads_fv3: 2
        threads_ufs: 2
        walltime: "06:00:00"

# Declare global GDAS resources for a nested case.
nested_global_gdas_atm_fcst_by_res: &NestedGdasFcstAtmRes
    C384:
        layout_x: 8
        layout_y: 8
        write_groups: 2
        write_tasks_per_group_per_thread_per_tile: 20
        walltime: "00:30:00"
    C768:
        layout_x: 16
        layout_y: 10
        write_groups: 2
        write_tasks_per_group_per_thread_per_tile: 90
        threads_fv3: 2
        threads_ufs: 2
        walltime: "00:30:00"

# Declare GFS nest resources for a nested case.
nested_nest_gfs_atm_fcst_by_res: &NestedNestFcstAtmRes
    C96:
        layout_x: 12
        layout_y: 10
        threads_fv3: 1
        threads_ufs: 1
        write_groups: 1
        write_tasks_per_group_per_thread_per_tile: 1
        walltime: "03:00:00"
    C192:
        layout_x: 4
        layout_y: 6
        threads_fv3: 2
        threads_ufs: 2
        write_groups: 2
        write_tasks_per_group_per_thread_per_tile: 5
        walltime: "03:00:00"
    C384:
        layout_x: 4
        layout_y: 6
        threads_fv3: 2
        threads_ufs: 2
        write_groups: 2
        write_tasks_per_group_per_thread_per_tile: 10
        walltime: "06:00:00"
    C768:
        layout_x: 12
        layout_y: 16
        write_groups: 4
        write_tasks_per_group_per_thread_per_tile: 20
        threads_fv3: 4
        threads_ufs: 4
        walltime: "06:00:00"


# Declare MOM6 forecast resources for each resolution.
# These are the same for gdas and gfs (fcst) and enkfgdas and enkfgfs (efcs) runs/jobs.
ocn_fcst_by_res: &FcstOcnRes
    500:
        num_PEs_mom6: 8
        threads_mom6: 1
    100:
        num_PEs_mom6: 20
        threads_mom6: 1
    050:
        num_PEs_mom6: 60
        threads_mom6: 1
    025:
        num_PEs_mom6: 220
        threads_mom6: 1
# Declare CICE6, also the same for gfs, gdas, enkfgdas, and enkfgfs
ice_fcst_by_res: &FcstICERes
    500:
        num_PEs_cice6: 4
        threads_cice6: 1
    100:
        num_PEs_cice6: 10
        threads_cice6: 1
    050:
        num_PEs_cice6: 30
        threads_cice6: 1
    025:
        num_PEs_cice6: 120
        threads_cice6: 1
# Declare WW3, also the same for gfs, gdas, enkfgdas, and enkfgfs
wave_fcst_by_res: &FcstWaveRes
    gnh_10m;aoc_9km;gsh_15m:
        ntasks_ww3: 140
        nthreads_ww3: 2
    gwes_30m:
        ntasks_ww3: 100
        nthreads_ww3: 2
    glo_025:
        ntasks_ww3: 262
        nthreads_ww3: 2
    glo_100:
        ntasks_ww3: 20
        nthreads_ww3: 1
    glo_200:
        ntasks_ww3: 30
        nthreads_ww3: 1
    glo_500:
        ntasks_ww3: 12
        nthreads_ww3: 1
    mx025:
        ntasks_ww3: 80
        nthreads_ww3: 2
    uglo_100km:
        ntasks_ww3: 40
        nthreads_ww3: 1
    uglo_m1g16:
        ntasks_ww3: 1000
        nthreads_ww3: 1

fcst:
    defaults:
        mem_per_node: "max"
        num_PEs: "ATMPETS + OCNPETS + ICEPETS + WAVPETS"

    # Different resources are needed for nested/non-nested experiments
    # See the definitions of these anchors above

    gdas:  # Options for GDAS forecasts
        {% if DO_NEST == True %}
        atmres: *NestedGdasFcstAtmRes
        {% else %}
        atmres: *GdasFcstAtmRes
        {% endif %}
        ocnres: *FcstOcnRes
        iceres: *FcstIceRes
        waveres: *FcstWaveRes

    gfs:  # Options for GFS forecasts
        {% if DO_NEST == True %}
        atmres: *NestedGfsFcstAtmRes
        {% else %}
        atmres: *GfsFcstAtmRes
        {% endif %}
        ocnres: *FcstOcnRes
        iceres: *FcstIceRes
        waveres: *FcstWaveRes

# Skipping efcs for now
efcs:
    defaults:
        mem_per_node: "max"
        USE_ESMF_THREADING: {{ USE_ESMF_THREADING }}

    # Ensemble settings
    # Different resources are needed for nested/non-nested experiments
    # See the definitions of these anchors above
    enkfgdas:  # Options for GDAS forecasts
        # Use the ensemble atmospheric resolution
        {% if DO_NEST == True %}
        atmensres: *NestedGdasFcstAtmRes
        {% else %}
        atmensres: *GdasFcstAtmRes
        {% endif %}
        # Ocean, ice, and waves ensemble configurations are identical to deterministic
        ocnres: *FcstOcnRes
        iceres: *FcstIceRes
        wavsres: *FcstWaveRes

    enkfgfs:  # Options for GFS forecasts
        # Use the ensemble atmospheric resolution
        {% if DO_NEST == True %}
        atmensres: *NestedGfsFcstAtmRes
        {% else %}
        atmensres: *GfsFcstAtmRes
        {% endif %}
        # Ocean, ice, and waves ensemble configurations are identical to deterministic
        ocnres: *FcstOcnRes
        iceres: *FcstIceRes
        wavsres: *FcstWaveRes
