# This jinja-yaml defines a resource dictionary for each task.
#
# Entries can be added to either the 'defaults' dictionary or valid run
# dictionaries (i.e. gdas, gfs, enkfgdas, enkfgfs) or both.
# For any entry duplicated in defaults and a run dictionary, the run
# dictionary will override the defaults.
#
# When needed, subdictionaries can be defined for each component and resolution
# to any of the dictionaries.  These will likewise override anything defined
# therein.
# For example:
#####
# task:
#     defaults:
#         walltime: "01:00:00"
#         num_PEs = "atm_PEs + ocn_PEs"
#         atmres:
#             C768:
#                 atm_PEs: 100
#             C384:
#                 atm_PEs: 40
#             C192:
#                 atm_PEs: 40
#             C96:
#                 atm_PEs: 20
#             C48:
#                 atm_PEs: 10
#
#         ocnres:
#             025:
#                 ocn_PEs: 20
#             050:
#                 ocn_PEs: 10
#             100:
#                 ocn_PEs: 5
#             500:
#                 ocn_PEs: 2
##### End #####
#
# Valid components are "atm", "atmens", "ocn", "ice", "wave", and "aero".
# Note that tasks starting with "e" will key off of the ensemble resolution for
# the component.  All others will key off of the deterministic resolution.
#
# Required entries for each task are
#    num_PEs (integer or string) - Number of process elements.
#              Should be an integer if fixed.
#              If the number of PEs is dependent on variables defined in the
#              dictionary, then the string can be used to represent that
#              calculation.  For instance, if the dictionary defines a layout
#              layout_x: 6, layout_y: 6
#              and the task requires six times the product of layout_x and
#              layout_y, then the string can be set to
#              "layout_x * layout_y * 6"
#              The string must be valid Python syntax and all operators must have
#              spaces between them.
#
#    walltime (string) - Maximum time required.
#                        Format: "HH:MM:SS"
#
# Optional entries that are used referenced at setup time
#    mem_per_PE - Memory required per PE.
#                 Default: mem_per_core * threads
#                 Formats:
#                         - NNN(.N)NB where NNN.N are numbers and S is one of
#                           G (giga), M (mega), K (kilo)
#                         - "max" if all available memory should be used
#                         - "default"
#
#    PEs_per_node - Maximum number of tasks per node.  This will be adjusted
#                   downward at runtime if the memory requirement is high.
#
#    mem_per_node - memory required per node.
#                   If not defined, then mem_per_PE is used.
#                   Formats are identical to mem_per_PE.
#
#    threads - Number of openMP threads per PE.
#            - NOTE this is only for openMP and should not be
#                   used to adjust memory.  Use mem_per_PE instead.
#
#    adjustable_threads - Boolean whether the thread count can be adjusted
#
# Additional entries may also be added.  Doing so will prepend the key/value
#    pair to the appropriate config file.  If the entry is in defaults, it
#    will be added as
#         export <key>=<value>
#    If the entry is in a run-specific dictionary, it will be added as
#         if [[ ${RUN} == <run> ]]; then
#             export <key>=<value>
#         fi
#
# Lastly, if machine-specific task defaults need to be specified, these may
#    be added to machines/<machine>_task_resources.yaml.j2.  Any task specified
#    in this way will override the definitions in this file.  In almost all cases,
#    this is not necessary except perhaps for the GSI analysis (task "anal").

# Create a namespace of machine specifics then update them by including the machine yaml

prep:
    defaults:
        walltime: "00:30:00"
        num_PEs: 4
        PEs_per_node: 2
        mem_per_node: "40GB"

prepsnowobs:
    defaults:
        walltime: "00:05:00"
        num_PEs: 1
        mem_per_node: "max"

prepatmiodaobs:
    defaults:
        walltime: "00:30:00"
        num_PEs: 1
        mem_per_node: "max"

aerosol_init:
    defaults:
        walltime: "00:05:00"
        num_PEs: 1
        mem_per_node: "6GB"

waveinit:
    defaults:
        walltime: "00:10:00"
        num_PEs: 12
        mem_per_PE: "200MB"

waveprep:
    defaults:
        walltime: "00:10:00"

    gfs:
        num_PEs: 65
        mem_per_PE: "2400MB"
    gdas:
        num_PEs: 5
        mem_per_PE: "20GB"

wavepostsbs:
    defaults:
        num_PEs: 8
        mem_per_PE: "1300MB"

    gdas:
        walltime: "00:20:00"
    gfs:
        walltime: "03:00:00"

wavepostbndpnt:
    defaults:
        walltime: "01:00:00"
        num_PEs: 240

wavepostbndpntbll:
    defaults:
        walltime: "01:00:00"
        num_PEs: 448

wavepostpnt:
    defaults:
        walltime: "04:00:00"
        num_PEs: 200
        PEs_per_node: 40

wavegempak:
    defaults:
        walltime: "02:00:00"
        num_PEs: 1
        mem_per_node: "1GB"

waveawipsbulls:
    defaults:
        walltime: "00:20:00"
        num_PEs: 1

waveawipsgridded:
    defaults:
        walltime: "02:00:00"
        num_PEs: 1
        mem_per_node: "1GB"

atmanlinit:
    defaults:
        walltime: "00:10:00"
        num_PEs: 1
        mem_per_node: "3GB"
        layout_x: @LAYOUT_X_ATMANL@
        layout_y: @LAYOUT_Y_ATMANL@
        layout_gsib_x: {{ @IO_LAYOUT_X@ * 3 }}
        layout_gsib_y: {{ @IO_LAYOUT_Y@ * 2 }}

atmanlvar:
    defaults:
        walltime: "00:30:00"
        num_PEs: {{ @LAYOUT_X_ATMANL@ * @LAYOUT_Y_ATMANL@ * 6 }}
        layout_x: @LAYOUT_X_ATMANL@
        layout_y: @LAYOUT_Y_ATMANL@
        layout_io_x: @IO_LAYOUT_X@
        layout_io_y: @IO_LAYOUT_Y@

atmanlfv3inc:
    defaults:
        walltime: "00:30:00"
        num_PEs: {{ @LAYOUT_X_ATMANL@ * @LAYOUT_Y_ATMANL@ * 6 }}
        layout_x: @LAYOUT_X_ATMANL@
        layout_y: @LAYOUT_Y_ATMANL@
        layout_io_x: @IO_LAYOUT_X@
        layout_io_y: @IO_LAYOUT_Y@

atmanlfinal:
    defaults:
        num_PEs: {{ host_info.cores_per_node }}
        walltime: "00:30:00"
        mem_per_node: "max"

snowanl:
    defaults:
        num_PEs: "layout_x * layout_y * 6"
        walltime: "00:15:00"
        layout_io_x: {{ @IO_LAYOUT_X@ }}
        layout_io_y: {{ @IO_LAYOUT_Y@ }}

        atmres:
            C768:
                layout_x: 6
                layout_y: 6

            C384:
                layout_x: 5
                layout_y: 5

            C192:
                layout_x: 1
                layout_y: 1

            C96:
                layout_x: 1
                layout_y: 1

            C48:
                layout_x: 1
                layout_y: 1

aeroanlinit:
    defaults:
        num_PEs: 1
        walltime: "00:10:00"
        mem_per_PE: "3GB"

        atmres:
            C768:
                layout_x: 8
                layout_y: 8

            C384:
                layout_x: 8
                layout_y: 8

            C192:
                layout_x: 8
                layout_y: 8

            C96:
                layout_x: 8
                layout_y: 8

            C48:
                layout_x: 1
                layout_y: 1

aeroanlrun:
    defaults:
        walltime: "00:30:00"
        num_PEs: "layout_x * layout_y * 6"

        atmres:
            C768:
                layout_x: 8
                layout_y: 8

            C384:
                layout_x: 8
                layout_y: 8

            C192:
                layout_x: 8
                layout_y: 8

            C96:
                layout_x: 8
                layout_y: 8

            C48:
                layout_x: 1
                layout_y: 1

aeroanlfinal:
    defaults:
        num_PEs: 1
        walltime: "00:10:00"
        mem_per_node: "3GB"

ocnanalprep:
    defaults:
        num_PEs: 1
        walltime: "00:10:00"
        mem_per_node: "24GB"

prepoceanobs:
    defaults:
        num_PEs: 1
        walltime: "00:10:00"
        mem_per_node: "48GB"

ocnanalbmat:
    defaults:
        walltime: "00:15:00"

        ocnres:
            025:
                num_PEs: 480
            050:
                num_PEs: 16
            500:
                num_PEs: 16

ocnanalrun:
{% set walltime = "00:15:00" %}
{% if OCNRES == "025" %}
    {% set num_PEs, mem_per_PE = (480, "max") %}
{% elif OCNRES == "050" or OCNRES == "500" %}
    {% set num_PEs, mem_per_PE = (16, "max") %}
{% elif OCNRES == "050" or OCNRES == "500" %}
    {% set num_PEs, mem_per_PE = (16, "1600MB") %}
{% else %}
    {% set num_PEs, mem_per_PE, walltime = (0, 0, "00:00:00") %}
{% endif %}

    defaults:
        walltime: "00:15:00"

        ocnres:
            025:
                num_PEs: 480
                {% if machine == "jet" %}
                mem_per_node: "max"
                {% else %}
                mem_per_node: "96GB"
                {% endif %}
            050:
                num_PEs: 16
                {% if machine == "jet" %}
                mem_per_node: "max"
                {% else %}
                mem_per_node: "96GB"
                {% endif %}
            500:
                num_PEs: 16
                mem_per_node: "24GB"

ocnanalecen:
    defaults:
        walltime: "00:10:00"
        ocnres:
            025:
                num_PEs: 40
                {% if machine == "jet" %}
                mem_per_node: "max"
                {% else %}
                mem_per_node: "96GB"
                {% endif %}
            050:
                num_PEs: 16
                {% if machine == "jet" %}
                mem_per_node: "max"
                {% else %}
                mem_per_node: "96GB"
                {% endif %}
            500:
                num_PEs: 16
                mem_per_node: "24GB"

ocnanalletkf:
    defaults:
        walltime: "00:10:00"
        ocnres:
            025:
                num_PEs: 480
                {% if machine == "jet" %}
                mem_per_node: "max"
                {% else %}
                mem_per_node: "96GB"
                {% endif %}
            050:
                num_PEs: 16
                {% if machine == "jet" %}
                mem_per_node: "max"
                {% else %}
                mem_per_node: "96GB"
                {% endif %}
            500:
                num_PEs: 16
                mem_per_node: "24GB"

ocnanalchkpt:
    defaults:
        walltime: "00:10:00"
        ocnres:
            025:
                mem_per_PE: "3200MB"
                num_PEs: 40
            050:
                mem_per_node: "32GB"
                num_PEs: 16
            500:
                mem_per_node: "32GB"
                num_PEs: 8

ocnanalpost:
    defaults:
        walltime: "00:30:00"
        num_PEs: {{ host_info.cores_per_node }}

ocnanalvrfy:
    defaults:
        walltime: "00:35:00"
        num_PEs: 1
        mem_per_node: "24GB"

anal:
    defaults:
        mem_per_node: "max"
        # Defaults on all systems but S4
        threads: 5
        num_PEs: 84

        # Defaults for S4
        {% if machine == "s4" %}
        threads: 4
            {% if partition == "s4" %}
        num_PEs: 88
            {% elif partition == "ivy" %}
        num_PEs: 90
            {% endif %}
        {% endif %}

        atmres:
            C768:
                num_PEs: 780
                threads: 5
            C384:
                {% if machine == "hera" or machine == "jet" %}
                threads: 8
                num_PEs: 270
                {% elif machine == "s4" %}
                threads: 1
                num_PEs: 416
                {% else %}
                threads: 10
                num_PEs: 160
                {% endif %}

            # Use defaults for C192, C96, and C48
            C192: {}
            C96: {}
            C48: {}

    gdas:
        wallclock: "01:20:00"

    gfs:
        wallclock: "01:00:00"
        atmres:
            # PE count is higher for GFS @C768
            C768:
                num_PEs: 825
            # Use defaults for all other resolutions
            C384: {}
            C192: {}
            C96:  {}
            C48:  {}

analcalc:
    # The analcalc job consists of a few sub jobs, each requiring a different
    # number of tasks.
    # echgres is run as part of analcalc and needs a different layout.
    #    - The number of tasks is the length of IAUFHRS.

    gdas:
        nth_echgres: 4

    gfs:
        # TODO Determine if this level of threading is really needed or just to resolve memory
        nth_echgres: 12

    defaults:
        walltime: "00:10:00"
        num_PEs: 127
        mem_per_PE: "1200MB"

analdiag:
    defaults:
        walltime: "00:15:00"
        num_PEs: 96             # Should be at least twice npe_ediag
        mem_per_PE: "1200MB"

sfcanl:
    defaults:
        walltime: "00:20:00"
        num_PEs: 6
        mem_per_PE: "15G"

fcst:
    # Different resources are needed for nested/non-nested experiments
    {% if DO_NEST == True %}
    {% else %}
    defaults:
        mem_per_node: "max"

    gfs:
        atmres:
            C48:
                layout_x: 1
                layout_y: 1
                threads_ufs: 1
                threads_fv3: 1
                write_groups: 1
                write_tasks_per_group_per_thread_per_tile: 1
                walltime: "03:00:00"
            C96:
                layout_x: 2
                layout_y: 2
                threads_fv3: 1
                threads_ufs: 1
                write_groups: 1
                write_tasks_per_group_per_thread_per_tile: 1
                walltime: "03:00:00"
            C192:
                layout_x: 4
                layout_y: 6
                threads_fv3: 2
                threads_ufs: 2
                write_groups: 2
                write_tasks_per_group_per_thread_per_tile: 5
                walltime: "03:00:00"
            C384:
                layout_x: 4
                layout_y: 6
                threads_fv3: 2
                threads_ufs: 2
                write_groups: 2
                write_tasks_per_group_per_thread_per_tile: 10
                walltime: "06:00:00"
            C768:
                layout_x: 12
                layout_y: 16
                write_groups: 4
                write_tasks_per_group_per_thread_per_tile: 20
                threads_fv3: 4
                threads_ufs: 4
                walltime: "06:00:00"
    gdas:
        defaults:
            walltime: "00:20:00"

        atmres:
            C48:
                layout_x: 1
                layout_y: 1
                threads_ufs: 1
                threads_fv3: 1
                write_groups: 1
                write_tasks_per_group_per_thread_per_tile: 1
                walltime: "00:20:00"
            C96:
                layout_x: 2
                layout_y: 2
                threads_fv3: 1
                threads_ufs: 1
                write_groups: 1
                write_tasks_per_group_per_thread_per_tile: 1
                walltime: "00:20:00"
            C192:
                layout_x: 4
                layout_y: 6
                threads_fv3: 1
                threads_ufs: 1
                write_groups: 1
                write_tasks_per_group_per_thread_per_tile: 10
                walltime: "00:20:00"
            C384:
                layout_x: 4
                layout_y: 6
                threads_fv3: 2
                threads_ufs: 2
                write_groups: 2
                write_tasks_per_group_per_thread_per_tile: 10
                walltime: "00:30:00"
            C768:
                walltime: "00:30:00"
                layout_x: 8
                layout_y: 12
                write_groups: 2
                threads_fv3: 4
                threads_ufs: 4
                write_tasks_per_group_per_thread_per_tile: 10
    {% endif %}

# Skipping efcs for now
efcs:
    defaults:
        mem_per_node: "max"

    # Ensemble settings

    enkfgfs:
    enkfgdas:

oceanice_products:
    defaults:
        walltime: "00:15:00"
        num_PEs: 1
        mem_per_node: "max"

upp:
    defaults:
        walltime: "00:15:00"
        atmres:
            C768:
                num_PEs: 120
                mem_per_node: "max"
            C384:
                num_PEs: 120
                mem_per_node: "max"
            C192:
                num_PEs: 120
                mem_per_node: "max"
            C96:
                num_PEs: 96
            C48:
                num_PEs: 48

atmos_products:
    defaults:
        walltime: "00:15:00"
        num_PEs: 24
        mem_per_PE: "2600MB"

verfozn:
    defaults:
        walltime: "00:05:00"
        num_PEs: 1
        mem_per_node: "1GB"

verfrad:
    defaults:
        walltime: "00:40:00"
        num_PEs: 1
        mem_per_node: "5GB"

vminmon:
    defaults:
        walltime: "00:05:00"
        num_PEs: 1
        mem_per_node: "1GB"

tracker:
    defaults:
        walltime: "00:10:00"
        num_PEs: 1
        mem_per_node: "4GB"

genesis:
    defaults:
        walltime: "00:25:00"
        num_PEs: 1
        mem_per_node: "10GB"

genesis_fsu:
    defaults:
        walltime: "00:10:00"
        num_PEs: 1
        mem_per_node: "1GB"

fit2obs:
    defaults:
        walltime: "00:20:00"
        num_PEs: 3
        mem_per_node: "20GB"

metp:
    gdas:
        walltime: "03:00:00"
    gfs:
        walltime: "06:00:00"

    defaults:
        num_PEs: 4
        mem_per_PE: "20GB"

echgres:
    defaults:
        walltime: "00:10:00"
        num_PEs: 3
        mem_per_PE: "20GB"

init:
    defaults:
        walltime: "00:30:00"
        num_PEs: 24
        mem_per_PE: "12GB"

init_chem:
    defaults:
        walltime: "00:30:00"
        num_PEs: 1
        mem_per_node: "max"

mom6ic:
    defaults:
        walltime: "00:30:00"
        num_PEs: 24
        mem_per_PE: "3600MB"

arch:
    defaults:
        walltime: "06:00:00"
        num_PEs: 1
        mem_per_node: "4GB"

earc:
    defaults:
        walltime: "06:00:00"
        num_PEs: 1
        mem_per_node: "4GB"

getic:
    defaults:
        walltime: "06:00:00"
        num_PEs: 1
        mem_per_node: "4GB"

cleanup:
    defaults:
        walltime: "00:15:00"
        num_PEs: 1
        mem_per_node: "4GB"

stage_ic:
    defaults:
        walltime: "00:15:00"
        num_PEs: 1
        mem_per_node: "max"

atmensanlinit:
    defaults:
        layout_x: @LAYOUT_X_ATMENSANL@
        layout_y: @LAYOUT_Y_ATMENSANL@

        walltime: "00:10:00"
        num_PEs: 1
        mem_per_node: "3GB"

atmensanlrun:
    defaults:
        layout_x: @LAYOUT_X_ATMENSANL@
        layout_y: @LAYOUT_Y_ATMENSANL@

        walltime: "00:30:00"
        num_PEs: {{ @LAYOUT_X_ATMENSANL@ * @LAYOUT_Y_ATMENSANL@ * 6 }}

atmensanlfinal:
    defaults:
        walltime: "00:30:00"
        num_PEs: 1
        mem_per_node: "max"

eomg:
    defaults:
        walltime: "00:30:00"
        threads: 2
        atmensres:
            C384:
                num_PEs: 200
            C192:
                num_PEs: 100
            C96:
                num_PEs: 40
            C48:
                num_PEs: 40

eobs:
    defaults:
        walltime: "00:15:00"
        threads: 2

        atmensres:
            C384:
                num_PEs: 200
            C192:
                num_PEs: 100
            C96:
                num_PEs: 40
            C48:
                num_PEs: 40

ediag:
    defaults:
        walltime: "00:15:00"
        num_PEs: 48
        mem_per_PE: "1300MB"

eupd:
    defaults:
        walltime: "00:30:00"

        atmres:
            C768:
                mem_per_PE: "24GB"
                num_PEs: 480
            C384:
                mem_per_PE: "12GB"
                num_PEs: 270
            C96:
                mem_per_PE: "9GB"
                num_PEs: 42
            C48:
                mem_per_PE: "9GB"
                num_PEs: 42

ecen:
    defaults:
        walltime: "00:10:00"
        num_PEs: 80
        mem_per_PE: "4700MB"

        atmensres:
            C384:
                mem_per_PE: "16GB"
            C192:
                mem_per_PE: "4700MB"
            C96:
                mem_per_PE: "4700MB"
            C48:
                mem_per_PE: "4700MB"

esfc:
    defaults:
        walltime: "00:15:00"
        num_PEs: 80
        mem_per_node: "max"

epos:
    defaults:
        walltime: "00:15:00"
        num_PEs: 80

postsnd:
    defaults:
        walltime: "02:00:00"
        num_PEs: 40
        mem_per_PE: "9GB"
        num_PEs_postsndcfp: 9

awips:
    defaults:
        walltime: "03:30:00"
        num_PEs: 1
        mem_per_node: "3GB"

npoess:
    defaults:
        walltime: "03:30:00"
        num_PEs: 1
        mem_per_node: "3GB"

gempak:
    defaults:
        walltime: "03:00:00"

    gdas:
        num_PEs: 2
        mem_per_PE: "2GB"
    gfs:
        num_PEs: 28
        mem_per_PE: "100MB"

mos_stn_prep:
    defaults:
        walltime: "00:10:00"
        num_PEs: 3
        mem_per_PE: "2GB"
        PTILE: 3

mos_grd_prep:
    defaults:
        walltime: "00:10:00"
        num_PEs: 4
        mem_per_PE: "4GB"
        PTILE: 4

mos_ext_stn_prep:
    defaults:
        walltime: "00:15:00"
        num_PEs: 2
        mem_per_PE: "3GB"
        PTILE: 2

mos_ext_grd_prep:
    defaults:
        walltime: "00:10:00"
        num_PEs: 7
        mem_per_PE: "500MB"
        PTILE: 7

mos_stn_fcst:
    defaults:
        walltime: "00:10:00"
        num_PEs: 5
        mem_per_PE: "8GB"
        PTILE: 5

mos_grd_fcst:
    defaults:
        walltime: "00:10:00"
        num_PEs: 7
        mem_per_PE: "7GB"
        PTILE: 7

mos_ext_stn_fcst:
    defaults:
        walltime: "00:20:00"
        num_PEs: 3
        mem_per_PE: "16600MB"
        PTILE: 3
        prepost: True

mos_ext_grd_fcst:
    defaults:
        walltime: "00:10:00"
        num_PEs: 7
        mem_per_PE: "7GB"
        PTILE: 7

mos_stn_prdgen:
    defaults:
        walltime: "00:10:00"
        num_PEs: 1
        mem_per_node: "15GB"
        PTILE: 1
        prepost: True

mos_grd_prdgen:
    defaults:
        walltime: "00:40:00"
        num_PEs: 72
        threads: 4
        mem_per_PE: "2GB"
        PTILE: 72

mos_ext_stn_prdgen:
    defaults:
        walltime: "00:10:00"
        num_PEs: 1
        mem_per_node: "15GB"
        PTILE: 1
        prepost: True

mos_ext_grd_prdgen:
    defaults:
        walltime: "00:30:00"
        num_PEs: 96
        threads: 16
        mem_per_PE: "15GB"
        PTILE: 96

mos_wx_prdgen:
    defaults:
        walltime: "00:10:00"
        num_PEs: 4
        threads: 2
        mem_per_PE: "2600MB"
        PTILE: 4

mos_wx_ext_prdgen:
    defaults:
        walltime: "00:10:00"
        num_PEs: 4
        threads: 2
        mem_per_PE: "2600MB"
        PTILE: 4
