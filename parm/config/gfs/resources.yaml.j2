{% include machine_ID + ".yaml.j2" %}
{% include "common.yaml.j2" %}
{% set var = 0 %}

prep:
    all_runs:
        walltime: "00:30:00"
        num_tasks: 4
        threads: 1
        mem_per_task: "20GB"

prepsnowobs:
    all_runs:
        walltime: "00:05:00"
        num_tasks: 1
        threads: 1
        mem_per_task: "0"

prepatmiodaobs:
    all_runs:
        walltime: "00:30:00"
        num_tasks: 1
        threads: 1
        mem_per_task: "0"

aerosol_init:
    all_runs:
        walltime: "00:05:00"
        num_tasks: 1
        threads: 1
        mem_per_task: "6GB"

waveinit:
    all_runs:
        walltime: "00:10:00"
        num_tasks: 12
        threads: 1
        mem_per_task: "200MB"

waveprep:
    all_runs:
        walltime: "00:10:00"
        threads: 1
    gfs:
        num_tasks: 65
        mem_per_task: "2500MB"
    gdas:
        num_tasks: 5
        mem_per_task: "20GB"

wavepostsbs:
    all_runs:
        num_tasks: 8
        threads: 1
        mem_per_task: "1300MB"
    gdas:
        walltime: "00:20:00"
    gfs:
        walltime: "03:00:00"

wavepostbndpnt:
    all_runs:
        walltime: "01:00:00"
        num_tasks: 240
        threads: 1
        mem_per_task: {{ mem_per_cpu }}

wavepostbndpntbll:
    all_runs:
        walltime: "01:00:00"
        num_tasks: 448
        threads: 1
        mem_per_task: {{ mem_per_cpu }}

wavepostpnt:
    all_runs:
        walltime: "04:00:00"
        num_tasks: 200
        threads: 1
        mem_per_task: {{ mem_per_cpu }}

wavegempak:
    all_runs:
        walltime: "02:00:00"
        num_tasks: 1
        threads: 1
        mem_per_task: "1GB"

waveawipsbulls:
    all_runs:
        walltime: "00:20:00"
        num_tasks: 1
        threads: 1
        mem_per_task: {{ mem_per_cpu }}

waveawipsgridded:
    all_runs:
        walltime: "02:00:00"
        num_tasks: 1
        threads: 1
        mem_per_task: "1GB"

atmanlinit:
    all_runs:
        walltime: "00:10:00"
        num_tasks: 1
        threads: 1
        mem_per_task: "3072M"

atmanlvar:
    all_runs:
        walltime: "00:30:00"
        num_tasks: {{ LAYOUT_X_ATMANL * LAYOUT_Y_ATMANL * 6 }}
        threads: 1
        mem_per_task: {{ mem_per_cpu }}
        layout:
            x: {{ LAYOUT_X_ATMANL }}
            y: {{ LAYOUT_Y_ATMANL }}
            io_x: {{ IO_LAYOUT_X }}
            io_y: {{ IO_LAYOUT_Y }}

atmanlfv3inc:
    all_runs:
        walltime: "00:30:00"
        num_tasks: {{ LAYOUT_X_ATMANL * LAYOUT_Y_ATMANL * 6 }}
        threads: 1
        mem_per_task: {{ mem_per_cpu }}
        layout:
            x: {{ LAYOUT_X_ATMANL }}
            y: {{ LAYOUT_Y_ATMANL }}
            io_x: {{ IO_LAYOUT_X }}
            io_y: {{ IO_LAYOUT_Y }}

atmanlfinal:
    all_runs:
        num_tasks: {{ cpus_per_node }}
        walltime: "00:30:00"
        threads: 1
        mem_per_task: {{ mem_per_cpu }}

snowanl:
    {% if CASE == "C768" %}
    {% set layout_x_snowanl, layout_y_snowanl == (6, 6) %}
    {% elif CASE == "C384" %}
    {% set layout_x_snowanl, layout_y_snowanl == (5, 5) %}
    {% elif CASE == "C192" or CASE == "C96" or CASE == "C48" %}
    {% set layout_x_snowanl, layout_y_snowanl == (1, 1) %}
    {% endif %}
    all_runs:
        num_tasks: {{ layout_x_snowanl * layout_y_snowanl * 6 }}
        walltime: "00:15:00"
        threads: 1
        mem_per_task: {{ mem_per_cpu }}
        layout:
            x: {{ layout_x_snowanl }}
            y: {{ layout_y_snowanl }}
            io_x: {{ IO_LAYOUT_X }}
            io_y: {{ IO_LAYOUT_Y }}

aeroanlinit:
    {% if CASE == "C768" %}
    {% set layout_x_aeroanlinit, layout_y_aeroanlinit == (8, 8) %}
    {% elif CASE == "C384" %}
    {% set layout_x_aeroanlinit, layout_y_aeroanlinit == (8, 8) %}
    {% elif CASE == "C192" or CASE == "C96" %}
    {% set layout_x_aeroanlinit, layout_y_aeroanlinit == (8, 8) %}
    {% elif CASE == "C48" %}
    {% set layout_x_aeroanlinit, layout_y_aeroanlinit == (1, 1) %}
    {% endif %}
    all_runs:
        num_tasks: 1
        walltime: "00:10:00"
        threads: 1
        mem_per_task: "3072M"
        layout:
            x: {{ layout_x_aeroanlinit }}
            y: {{ layout_y_aeroanlinit }}

aeroanlrun:
    {% if CASE == "C768" %}
    {% set layout_x_aeroanlrun, layout_y_aeroanlrun == (8, 8) %}
    {% elif CASE == "C384" %}
    {% set layout_x_aeroanlrun, layout_y_aeroanlrun == (8, 8) %}
    {% elif CASE == "C192" or CASE == "C96" %}
    {% set layout_x_aeroanlrun, layout_y_aeroanlrun == (8, 8) %}
    {% elif CASE == "C48" %}
    {% set layout_x_aeroanlrun, layout_y_aeroanlrun == (1, 1) %}
    {% endif %}
    all_runs:
        num_tasks: {{ layout_x_aeroanlrun * layout_y_aeroanlrun * 6 }}
        walltime: "00:30:00"
        threads: 1
        mem_per_task: {{ mem_per_cpu }}
        layout:
            x: {{ layout_x_aeroanlrun }}
            y: {{ layout_y_aeroanlrun }}

aeroanlfinal:
    all_runs:
        num_tasks: 1
        walltime: "00:10:00"
        threads: 1
        mem_per_task: "3072M"

ocnanalprep:
    all_runs:
        num_tasks: 1
        walltime: "00:10:00"
        threads: 1
        mem_per_task: "24GB"

prepoceanobs:
    all_runs:
        num_tasks: 1
        walltime: "00:10:00"
        threads: 1
        mem_per_task: "48GB"

ocnanalbmat:
    {% if OCNRES == "025" %}
    {% set tasks_ocnanalbmat = 480 %}
    {% elif OCNRES == "050" %}
    {% set tasks_ocnanalbmat = 16 %}
    {% elif OCNRES == "500" %}
    {% set tasks_ocnanalbmat = 16 %}
    {% endif %}
    all_runs:
        num_tasks: {{ tasks_ocnanalbmat }}
        walltime: "00:15:00"
        threads: 1
        mem_per_task: {{ mem_per_cpu }}

ocnanalecen:
    {% elif OCNRES == "025" %}
    {% set npes = 40 %}
    {% set memory_ocnanalecen = "96GB" %}
    {% endif %}
    {% elif OCNRES == "050" %}
    {% set npes = 16 %}
    {% set memory_ocnanalecen = "96GB" %}
    {% endif %}
    {% elif OCNRES == "500" %}
    {% set npes = 16 %}
    {% set memory_ocnanalecen = "24GB" %}
    {% else %}
    {% set npes = 16 %}
    {% endif %}

    wtime_ocnanalecen="00:10:00"
    npe_ocnanalecen=${npes}
    nth_ocnanalecen=1
    is_exclusive=True
    npe_node_ocnanalecen=$(( npe_node_max / nth_ocnanalecen ))
    memory_ocnanalecen

ocnanalchkpt:
    wtime_ocnanalchkpt="00:10:00"
    npe_ocnanalchkpt=1
    nth_ocnanalchkpt=1
    npe_node_ocnanalchkpt=$(( npe_node_max / nth_ocnanalchkpt ))
    case ${OCNRES} in
      {% if OCNRES == "025" %}
        memory_ocnanalchkpt="128GB"
        npes=40
        {% endif %}
      {% if OCNRES == "050" %}
        memory_ocnanalchkpt="32GB"
        npes=16
        {% endif %}
      {% if OCNRES == "500" %}
        memory_ocnanalchkpt="32GB"
        npes=8
        {% endif %}
        echo "FATAL ERROR: Resources not defined for job ${job} at resolution ${OCNRES}"
        exit 4
    esac
    npe_ocnanalchkpt=${npes}
    memory_ocnanalchkpt

ocnanalpost:
    wtime_ocnanalpost="00:30:00"
    npe_ocnanalpost=${npe_node_max}
    nth_ocnanalpost=1
    npe_node_ocnanalpost=$(( npe_node_max / nth_ocnanalpost ))

ocnanalvrfy:
    wtime_ocnanalvrfy="00:35:00"
    npe_ocnanalvrfy=1
    nth_ocnanalvrfy=1
    npe_node_ocnanalvrfy=$(( npe_node_max / nth_ocnanalvrfy ))
    memory_ocnanalvrfy="24GB"

anal:
    wtime_anal="01:20:00"
    wtime_anal_gfs="01:00:00"
    npe_anal=780
    nth_anal=5
    npe_anal_gfs=825
    nth_anal_gfs=5
    if [[ "${machine}" == "WCOSS2" ]]; then
      nth_anal=8
      nth_anal_gfs=8
    fi
    case ${CASE} in
      "C384")
        npe_anal=160
        npe_anal_gfs=160
        nth_anal=10
        nth_anal_gfs=10
        if [[ ${machine} = "HERA" ]]; then
          npe_anal=270
          npe_anal_gfs=270
          nth_anal=8
          nth_anal_gfs=8
        fi	  
        if [[ ${machine} = "S4" ]]; then
          #On the S4-s4 partition, this is accomplished by increasing the task
          #count to a multiple of 32
          if [[ ${PARTITION_BATCH} = "s4" ]]; then
            npe_anal=416
            npe_anal_gfs=416
          fi
          #S4 is small, so run this task with just 1 thread
          nth_anal=1
          nth_anal_gfs=1
          wtime_anal="02:00:00"
        fi
        {% endif %}
      "C192" | "C96" | "C48")
        npe_anal=84
        npe_anal_gfs=84
        if [[ ${machine} == "S4" ]]; then
          nth_anal=4
          nth_anal_gfs=4
          #Adjust job count for S4
          if [[ ${PARTITION_BATCH} == "s4" ]]; then
            npe_anal=88
            npe_anal_gfs=88
          elif [[ ${PARTITION_BATCH} == "ivy" ]]; then
            npe_anal=90
            npe_anal_gfs=90
          fi
        fi
        {% endif %}
        echo "FATAL ERROR: Resources not defined for job ${job} at resolution ${CASE}"
        exit 4
        {% endif %}
    esac
    npe_node_anal=$(( npe_node_max / nth_anal ))
    nth_cycle=${nth_anal}
    npe_node_cycle=$(( npe_node_max / nth_cycle ))
    is_exclusive=True

analcalc:
    wtime_analcalc="00:10:00"
    npe_analcalc=127
    ntasks="${npe_analcalc}"
    nth_analcalc=1
    nth_echgres=4
    nth_echgres_gfs=12
    npe_node_analcalc=$(( npe_node_max / nth_analcalc ))
    is_exclusive=True
    memory_analcalc="48GB"

analdiag:
    wtime_analdiag="00:15:00"
    npe_analdiag=96             # Should be at least twice npe_ediag
    nth_analdiag=1
    npe_node_analdiag=$(( npe_node_max / nth_analdiag ))
    memory_analdiag="48GB"

sfcanl:
    wtime_sfcanl="00:20:00"
    npe_sfcanl=6
    nth_sfcanl=1
    npe_node_sfcanl=$(( npe_node_max / nth_sfcanl ))
    is_exclusive=True

fcst" | "efcs:
    is_exclusive=True

    if [[ "${step}" == "fcst" ]]; then
      _CDUMP_LIST=${CDUMP:-"gdas gfs"}
    elif [[ "${step}" == "efcs" ]]; then
      _CDUMP_LIST=${CDUMP:-"enkfgdas enkfgfs"}
    fi

    # During workflow creation, we need resources for all CDUMPs and CDUMP is undefined
    for _CDUMP in ${_CDUMP_LIST}; do
      if [[ "${_CDUMP}" =~ "gfs" ]]; then
        layout_x=${layout_x_gfs}
        layout_y=${layout_y_gfs}
        WRITE_GROUP=${WRITE_GROUP_GFS}
        WRTTASK_PER_GROUP_PER_THREAD=${WRTTASK_PER_GROUP_PER_THREAD_GFS}
        ntasks_fv3=${ntasks_fv3_gfs}
        ntasks_quilt=${ntasks_quilt_gfs}
        nthreads_fv3=${nthreads_fv3_gfs}
        nthreads_ufs=${nthreads_ufs_gfs}
      fi

      # Determine if using ESMF-managed threading or traditional threading
      # If using traditional threading, set them to 1
      if [[ "${USE_ESMF_THREADING:-}" == "YES" ]]; then
        UFS_THREADS=1
      else  # traditional threading
        UFS_THREADS=${nthreads_ufs:-1}
        nthreads_fv3=1
        nthreads_mediator=1
        [[ "${DO_WAVE}" == "YES" ]] && nthreads_ww3=1
        [[ "${DO_OCN}" == "YES" ]] && nthreads_mom6=1
        [[ "${DO_ICE}" == "YES" ]] && nthreads_cice6=1
      fi

      if (( ntiles > 6 )); then
        layout_x_nest=${layout_x_nest:-10}
        layout_y_nest=${layout_y_nest:-10}
        npx_nest=${npx_nest:-1441}
        npy_nest=${npy_nest:-961}
      fi

      # PETS for the atmosphere dycore
      (( FV3PETS = ntasks_fv3 * nthreads_fv3 ))
      echo "FV3 using (nthreads, PETS) = (${nthreads_fv3}, ${FV3PETS})"

      # PETS for quilting
      if [[ "${QUILTING:-}" == ".true." ]]; then
        (( QUILTPETS = ntasks_quilt * nthreads_fv3 ))
        (( WRTTASK_PER_GROUP = WRTTASK_PER_GROUP_PER_THREAD ))
        WRTTASK_PER_GROUP
      else
        QUILTPETS=0
      fi
      echo "QUILT using (nthreads, PETS) = (${nthreads_fv3}, ${QUILTPETS})"

      # Total PETS for the atmosphere component
      ATMTHREADS=${nthreads_fv3}
      (( ATMPETS = FV3PETS + QUILTPETS ))
      ATMPETS ATMTHREADS
      echo "FV3ATM using (nthreads, PETS) = (${ATMTHREADS}, ${ATMPETS})"

      # Total PETS for the coupled model (starting w/ the atmosphere)
      NTASKS_TOT=${ATMPETS}

      # The mediator PETS can overlap with other components, usually it lands on the atmosphere tasks.
      # However, it is suggested limiting mediator PETS to 300, as it may cause the slow performance.
      # See https://docs.google.com/document/d/1bKpi-52t5jIfv2tuNHmQkYUe3hkKsiG_DG_s6Mnukog/edit
      # TODO: Update reference when moved to ufs-weather-model RTD
      MEDTHREADS=${nthreads_mediator:-1}
      MEDPETS=${MEDPETS:-${FV3PETS}}
      (( "${MEDPETS}" > 300 )) && MEDPETS=300
      MEDPETS MEDTHREADS
      echo "MEDIATOR using (threads, PETS) = (${MEDTHREADS}, ${MEDPETS})"

      CHMPETS=0; CHMTHREADS=0
      if [[ "${DO_AERO}" == "YES" ]]; then
        # GOCART shares the same grid and forecast tasks as FV3 (do not add write grid component tasks).
        (( CHMTHREADS = ATMTHREADS ))
        (( CHMPETS = FV3PETS ))
        # Do not add to NTASKS_TOT
        echo "GOCART using (threads, PETS) = (${CHMTHREADS}, ${CHMPETS})"
      fi
      CHMPETS CHMTHREADS

      WAVPETS=0; WAVTHREADS=0
      if [[ "${DO_WAVE}" == "YES" ]]; then
        (( WAVPETS = ntasks_ww3 * nthreads_ww3 ))
        (( WAVTHREADS = nthreads_ww3 ))
        echo "WW3 using (threads, PETS) = (${WAVTHREADS}, ${WAVPETS})"
        (( NTASKS_TOT = NTASKS_TOT + WAVPETS ))
      fi
      WAVPETS WAVTHREADS

      OCNPETS=0; OCNTHREADS=0
      if [[ "${DO_OCN}" == "YES" ]]; then
        (( OCNPETS = ntasks_mom6 * nthreads_mom6 ))
        (( OCNTHREADS = nthreads_mom6 ))
        echo "MOM6 using (threads, PETS) = (${OCNTHREADS}, ${OCNPETS})"
        (( NTASKS_TOT = NTASKS_TOT + OCNPETS ))
      fi
      OCNPETS OCNTHREADS

      ICEPETS=0; ICETHREADS=0
      if [[ "${DO_ICE}" == "YES" ]]; then
        (( ICEPETS = ntasks_cice6 * nthreads_cice6 ))
        (( ICETHREADS = nthreads_cice6 ))
        echo "CICE6 using (threads, PETS) = (${ICETHREADS}, ${ICEPETS})"
        (( NTASKS_TOT = NTASKS_TOT + ICEPETS ))
      fi
      ICEPETS ICETHREADS

      echo "Total PETS for ${_CDUMP} = ${NTASKS_TOT}"

      if [[ "${_CDUMP}" =~ "gfs" ]]; then
        declare -x "npe_${step}_gfs"="${NTASKS_TOT}"
        declare -x "nth_${step}_gfs"="${UFS_THREADS}"
        declare -x "npe_node_${step}_gfs"="${npe_node_max}"
      else
        declare -x "npe_${step}"="${NTASKS_TOT}"
        declare -x "nth_${step}"="${UFS_THREADS}"
        declare -x "npe_node_${step}"="${npe_node_max}"
      fi

    done

    case "${CASE}" in
      "C48" | "C96" | "C192")
        declare -x "wtime_${step}"="00:15:00"
        declare -x "wtime_${step}_gfs"="03:00:00"
        {% endif %}
      "C384")
        declare -x "wtime_${step}"="00:30:00"
        declare -x "wtime_${step}_gfs"="06:00:00"
        {% endif %}
      "C768" | "C1152")
        declare -x "wtime_${step}"="00:30:00"
        declare -x "wtime_${step}_gfs"="06:00:00"
        {% endif %}
        echo "FATAL ERROR: Resources not defined for job ${job} at resolution ${CASE}"
        exit 4
        {% endif %}
    esac

    unset _CDUMP _CDUMP_LIST
    unset NTASKS_TOT

oceanice_products:
    wtime_oceanice_products="00:15:00"
    npe_oceanice_products=1
    npe_node_oceanice_products=1
    nth_oceanice_products=1
    memory_oceanice_products="96GB"

upp:
    case "${CASE}" in
      "C48" | "C96")
        npe_upp=${CASE:1}
      {% endif %}
      "C192" | "C384")
        npe_upp=120
        memory_upp="96GB"
      {% endif %}
      "C768")
        npe_upp=120
        memory_upp="96GB"
        if [[ ${machine} == "WCOSS2" ]]; then memory_upp="480GB" ; fi
      {% endif %}
        echo "FATAL ERROR: Resources not defined for job ${job} at resolution ${CASE}"
        exit 4
      {% endif %}
    esac
    npe_node_upp=${npe_upp}

    nth_upp=1

    wtime_upp="00:15:00"
    if (( npe_node_upp > npe_node_max )); then
      npe_node_upp=${npe_node_max}
    fi
    is_exclusive=True

atmos_products:
    wtime_atmos_products="00:15:00"
    npe_atmos_products=24
    nth_atmos_products=1
    npe_node_atmos_products="${npe_atmos_products}"
    wtime_atmos_products_gfs="${wtime_atmos_products}"
    npe_atmos_products_gfs="${npe_atmos_products}"
    nth_atmos_products_gfs="${nth_atmos_products}"
    npe_node_atmos_products_gfs="${npe_node_atmos_products}"
    is_exclusive=True

verfozn:
    wtime_verfozn="00:05:00"
    npe_verfozn=1
    nth_verfozn=1
    npe_node_verfozn=1
    memory_verfozn="1G"

verfrad:
    wtime_verfrad="00:40:00"
    npe_verfrad=1
    nth_verfrad=1
    npe_node_verfrad=1
    memory_verfrad="5G"

vminmon:
    wtime_vminmon="00:05:00"
    npe_vminmon=1
    nth_vminmon=1
    npe_node_vminmon=1
    wtime_vminmon_gfs="00:05:00"
    npe_vminmon_gfs=1
    nth_vminmon_gfs=1
    npe_node_vminmon_gfs=1
    memory_vminmon="1G"

tracker:
    wtime_tracker="00:10:00"
    npe_tracker=1
    nth_tracker=1
    npe_node_tracker=1
    memory_tracker="4G"

genesis:
    wtime_genesis="00:25:00"
    npe_genesis=1
    nth_genesis=1
    npe_node_genesis=1
    memory_genesis="10G"

genesis_fsu:
    wtime_genesis_fsu="00:10:00"
    npe_genesis_fsu=1
    nth_genesis_fsu=1
    npe_node_genesis_fsu=1
    memory_genesis_fsu="10G"

fit2obs:
    wtime_fit2obs="00:20:00"
    npe_fit2obs=3
    nth_fit2obs=1
    npe_node_fit2obs=1
    memory_fit2obs="20G"
    if [[ ${machine} == "WCOSS2" ]]; then npe_node_fit2obs=3 ; fi

metp:
    nth_metp=1
    wtime_metp="03:00:00"
    npe_metp=4
    npe_node_metp=4
    wtime_metp_gfs="06:00:00"
    npe_metp_gfs=4
    npe_node_metp_gfs=4
    is_exclusive=True

echgres:
    wtime_echgres="00:10:00"
    npe_echgres=3
    nth_echgres=${npe_node_max}
    npe_node_echgres=1
    if [[ "${machine}" == "WCOSS2" ]]; then
      memory_echgres="200GB"
    fi

init:
    wtime_init="00:30:00"
    npe_init=24
    nth_init=1
    npe_node_init=6
    memory_init="70GB"

init_chem:
    wtime_init_chem="00:30:00"
    npe_init_chem=1
    npe_node_init_chem=1
    is_exclusive=True

mom6ic:
    wtime_mom6ic="00:30:00"
    npe_mom6ic=24
    npe_node_mom6ic=24
    is_exclusive=True

arch" | "earc" | "getic:
    declare -x "wtime_${step}"="06:00:00"
    declare -x "npe_${step}"="1"
    declare -x "npe_node_${step}"="1"
    declare -x "nth_${step}"="1"
    declare -x "memory_${step}"="4096M"
    if [[ "${machine}" == "WCOSS2" ]]; then
      declare -x "memory_${step}"="50GB"
    fi

cleanup:
    wtime_cleanup="00:15:00"
    npe_cleanup=1
    npe_node_cleanup=1
    nth_cleanup=1
    memory_cleanup="4096M"

stage_ic:
    wtime_stage_ic="00:15:00"
    npe_stage_ic=1
    npe_node_stage_ic=1
    nth_stage_ic=1
    is_exclusive=True

atmensanlinit:
    layout_x=${layout_x_atmensanl}
    layout_y=${layout_y_atmensanl}

    wtime_atmensanlinit="00:10:00"
    npe_atmensanlinit=1
    nth_atmensanlinit=1
    npe_node_atmensanlinit=$(( npe_node_max / nth_atmensanlinit ))
    memory_atmensanlinit="3072M"

atmensanlrun:
    layout_x=${layout_x_atmensanl}
    layout_y=${layout_y_atmensanl}

    wtime_atmensanlrun="00:30:00"
    npe_atmensanlrun=$(( layout_x * layout_y * 6 ))
    npe_atmensanlrun_gfs=$(( layout_x * layout_y * 6 ))
    nth_atmensanlrun=1
    nth_atmensanlrun_gfs=${nth_atmensanlrun}
    npe_node_atmensanlrun=$(( npe_node_max / nth_atmensanlrun ))
    memory_atmensanlrun="96GB"
    is_exclusive=True

atmensanlfinal:
    wtime_atmensanlfinal="00:30:00"
    npe_atmensanlfinal=${npe_node_max}
    nth_atmensanlfinal=1
    npe_node_atmensanlfinal=$(( npe_node_max / nth_atmensanlfinal ))
    is_exclusive=True

eobs" | "eomg:
    wtime_eobs="00:15:00"
    wtime_eomg="00:30:00"
    case ${CASE} in
      "C768")                 npe_eobs=200
      {% endif %}
      "C384")                 npe_eobs=100
      {% endif %}
      "C192" | "C96" | "C48") npe_eobs=40
      {% endif %}
        echo "FATAL ERROR: Resources not defined for job ${job} at resolution ${CASE}"
        exit 4
        {% endif %}
    esac
    npe_eomg=${npe_eobs}
    nth_eobs=2
    nth_eomg=${nth_eobs}
    npe_node_eobs=$(( npe_node_max / nth_eobs ))
    is_exclusive=True
    # The number of tasks and cores used must be the same for eobs
    # See https://github.com/NOAA-EMC/global-workflow/issues/2092 for details
    # For S4, this is accomplished by running 10 tasks/node
    if [[ ${machine} = "S4" ]]; then
      npe_node_eobs=10
    elif [[ ${machine} = "HERCULES" ]]; then
      # For Hercules, this is only an issue at C384; use 20 tasks/node
      if [[ ${CASE} = "C384" ]]; then
        npe_node_eobs=20
      fi
    fi
    npe_node_eomg=${npe_node_eobs}

ediag:
    wtime_ediag="00:15:00"
    npe_ediag=48
    nth_ediag=1
    npe_node_ediag=$(( npe_node_max / nth_ediag ))
    memory_ediag="30GB"

eupd:
    wtime_eupd="00:30:00"
    case ${CASE} in
      "C768")
        npe_eupd=480
        nth_eupd=6
        if [[ "${machine}" == "WCOSS2" ]]; then
          npe_eupd=315
          nth_eupd=14
        fi
        {% endif %}
      "C384")
        npe_eupd=270
        nth_eupd=8
        if [[ "${machine}" == "WCOSS2" ]]; then
          npe_eupd=315
          nth_eupd=14
        elif [[ ${machine} == "S4" ]]; then
           npe_eupd=160
           nth_eupd=2
        fi
        {% endif %}
      "C192" | "C96" | "C48")
        npe_eupd=42
        nth_eupd=2
        if [[ "${machine}" == "HERA" || "${machine}" == "JET" ]]; then
          nth_eupd=4
        fi
        {% endif %}
        echo "FATAL ERROR: Resources not defined for job ${job} at resolution ${CASE}"
        exit 4
        {% endif %}
    esac
    npe_node_eupd=$(( npe_node_max / nth_eupd ))
    is_exclusive=True

ecen:
    wtime_ecen="00:10:00"
    npe_ecen=80
    nth_ecen=4
    if [[ "${machine}" == "HERA" ]]; then nth_ecen=6; fi
    if [[ ${CASE} == "C384" || ${CASE} == "C192" || ${CASE} == "C96" || ${CASE} == "C48" ]]; then
      nth_ecen=2
    fi
    npe_node_ecen=$(( npe_node_max / nth_ecen ))
    nth_cycle=${nth_ecen}
    npe_node_cycle=$(( npe_node_max / nth_cycle ))
    is_exclusive=True

esfc:
    wtime_esfc="00:15:00"
    npe_esfc=80
    nth_esfc=1
    npe_node_esfc=$(( npe_node_max / nth_esfc ))
    nth_cycle=${nth_esfc}
    npe_node_cycle=$(( npe_node_max / nth_cycle ))
    memory_esfc="80GB"

epos:
    wtime_epos="00:15:00"
    npe_epos=80
    nth_epos=1
    npe_node_epos=$(( npe_node_max / nth_epos ))
    is_exclusive=True

postsnd:
    wtime_postsnd="02:00:00"
    npe_postsnd=40
    nth_postsnd=8
    npe_node_postsnd=10
    npe_postsndcfp=9
    npe_node_postsndcfp=1
    postsnd_req_cores=$(( npe_node_postsnd * nth_postsnd ))
    if (( postsnd_req_cores > npe_node_max )); then
        npe_node_postsnd=$(( npe_node_max / nth_postsnd ))
    fi
    is_exclusive=True

awips:
    wtime_awips="03:30:00"
    npe_awips=1
    npe_node_awips=1
    nth_awips=1
    memory_awips="3GB"

npoess:
    wtime_npoess="03:30:00"
    npe_npoess=1
    npe_node_npoess=1
    nth_npoess=1
    memory_npoess="3GB"

gempak:
    wtime_gempak="03:00:00"
    npe_gempak=2
    npe_gempak_gfs=28
    npe_node_gempak=2
    npe_node_gempak_gfs=28
    nth_gempak=1
    memory_gempak="4GB"
    memory_gempak_gfs="2GB"

mos_stn_prep:
    wtime_mos_stn_prep="00:10:00"
    npe_mos_stn_prep=3
    npe_node_mos_stn_prep=3
    nth_mos_stn_prep=1
    memory_mos_stn_prep="5GB"
    NTASK="${npe_mos_stn_prep}"
    PTILE="${npe_node_mos_stn_prep}"

mos_grd_prep:
    wtime_mos_grd_prep="00:10:00"
    npe_mos_grd_prep=4
    npe_node_mos_grd_prep=4
    nth_mos_grd_prep=1
    memory_mos_grd_prep="16GB"
    NTASK="${npe_mos_grd_prep}"
    PTILE="${npe_node_mos_grd_prep}"

mos_ext_stn_prep:
    wtime_mos_ext_stn_prep="00:15:00"
    npe_mos_ext_stn_prep=2
    npe_node_mos_ext_stn_prep=2
    nth_mos_ext_stn_prep=1
    memory_mos_ext_stn_prep="5GB"
    NTASK="${npe_mos_ext_stn_prep}"
    PTILE="${npe_node_mos_ext_stn_prep}"

mos_ext_grd_prep:
    wtime_mos_ext_grd_prep="00:10:00"
    npe_mos_ext_grd_prep=7
    npe_node_mos_ext_grd_prep=7
    nth_mos_ext_grd_prep=1
    memory_mos_ext_grd_prep="3GB"
    NTASK="${npe_mos_ext_grd_prep}"
    PTILE="${npe_node_mos_ext_grd_prep}"

mos_stn_fcst:
    wtime_mos_stn_fcst="00:10:00"
    npe_mos_stn_fcst=5
    npe_node_mos_stn_fcst=5
    nth_mos_stn_fcst=1
    memory_mos_stn_fcst="40GB"
    NTASK="${npe_mos_stn_fcst}"
    PTILE="${npe_node_mos_stn_fcst}"

mos_grd_fcst:
    wtime_mos_grd_fcst="00:10:00"
    npe_mos_grd_fcst=7
    npe_node_mos_grd_fcst=7
    nth_mos_grd_fcst=1
    memory_mos_grd_fcst="50GB"
    NTASK="${npe_mos_grd_fcst}"
    PTILE="${npe_node_mos_grd_fcst}"

mos_ext_stn_fcst:
    wtime_mos_ext_stn_fcst="00:20:00"
    npe_mos_ext_stn_fcst=3
    npe_node_mos_ext_stn_fcst=3
    nth_mos_ext_stn_fcst=1
    memory_mos_ext_stn_fcst="50GB"
    NTASK="${npe_mos_ext_stn_fcst}"
    PTILE="${npe_node_mos_ext_stn_fcst}"
    prepost=True

mos_ext_grd_fcst:
    wtime_mos_ext_grd_fcst="00:10:00"
    npe_mos_ext_grd_fcst=7
    npe_node_mos_ext_grd_fcst=7
    nth_mos_ext_grd_fcst=1
    memory_mos_ext_grd_fcst="50GB"
    NTASK="${npe_mos_ext_grd_fcst}"
    PTILE="${npe_node_mos_ext_grd_fcst}"

mos_stn_prdgen:
    wtime_mos_stn_prdgen="00:10:00"
    npe_mos_stn_prdgen=1
    npe_node_mos_stn_prdgen=1
    nth_mos_stn_prdgen=1
    memory_mos_stn_prdgen="15GB"
    NTASK="${npe_mos_stn_prdgen}"
    PTILE="${npe_node_mos_stn_prdgen}"
    prepost=True

mos_grd_prdgen:
    wtime_mos_grd_prdgen="00:40:00"
    npe_mos_grd_prdgen=72
    npe_node_mos_grd_prdgen=18
    nth_mos_grd_prdgen=4
    memory_mos_grd_prdgen="20GB"
    NTASK="${npe_mos_grd_prdgen}"
    PTILE="${npe_node_mos_grd_prdgen}"
    OMP_NUM_THREADS="${nth_mos_grd_prdgen}"

mos_ext_stn_prdgen:
    wtime_mos_ext_stn_prdgen="00:10:00"
    npe_mos_ext_stn_prdgen=1
    npe_node_mos_ext_stn_prdgen=1
    nth_mos_ext_stn_prdgen=1
    memory_mos_ext_stn_prdgen="15GB"
    NTASK="${npe_mos_ext_stn_prdgen}"
    PTILE="${npe_node_mos_ext_stn_prdgen}"
    prepost=True

mos_ext_grd_prdgen:
    wtime_mos_ext_grd_prdgen="00:30:00"
    npe_mos_ext_grd_prdgen=96
    npe_node_mos_ext_grd_prdgen=6
    nth_mos_ext_grd_prdgen=16
    memory_mos_ext_grd_prdgen="30GB"
    NTASK="${npe_mos_ext_grd_prdgen}"
    PTILE="${npe_node_mos_ext_grd_prdgen}"
    OMP_NUM_THREADS="${nth_mos_ext_grd_prdgen}"

mos_wx_prdgen:
    wtime_mos_wx_prdgen="00:10:00"
    npe_mos_wx_prdgen=4
    npe_node_mos_wx_prdgen=2
    nth_mos_wx_prdgen=2
    memory_mos_wx_prdgen="10GB"
    NTASK="${npe_mos_wx_prdgen}"
    PTILE="${npe_node_mos_wx_prdgen}"
    OMP_NUM_THREADS="${nth_mos_wx_prdgen}"

mos_wx_ext_prdgen:
    wtime_mos_wx_ext_prdgen="00:10:00"
    npe_mos_wx_ext_prdgen=4
    npe_node_mos_wx_ext_prdgen=2
    nth_mos_wx_ext_prdgen=2
    memory_mos_wx_ext_prdgen="10GB"
    NTASK="${npe_mos_wx_ext_prdgen}"
    PTILE="${npe_node_mos_wx_ext_prdgen}"
    OMP_NUM_THREADS="${nth_mos_wx_ext_prdgen}"

    echo "FATAL ERROR: Invalid job ${step} passed to ${BASH_SOURCE[0]}"
    exit 1

esac

echo "END: config.resources"
